{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Random Enviroment in OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'DOWNRIGHT',\n",
       " 'DOWNLEFT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 score:210.0\n",
      "Episode:2 score:220.0\n",
      "Episode:3 score:330.0\n",
      "Episode:4 score:160.0\n",
      "Episode:5 score:210.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4,5,6,7,8])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(128, (8,8), strides=(4,4), activation='relu', input_shape=(3, height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 3, 51, 39, 128)    24704     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 24, 18, 64)     131136    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 11, 8, 64)      65600     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16896)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               8651264   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 9,006,345\n",
      "Trainable params: 9,006,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Agent with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg',\n",
    "                  nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMEER\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  569/10000: episode: 1, duration: 15.051s, episode steps: 569, steps per second:  38, episode reward: 110.000, mean reward:  0.193 [ 0.000, 10.000], mean action: 3.919 [0.000, 8.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMEER\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1260/10000: episode: 2, duration: 548.891s, episode steps: 691, steps per second:   1, episode reward: 190.000, mean reward:  0.275 [ 0.000, 10.000], mean action: 3.767 [0.000, 8.000],  loss: 22.684262, mean_q: 5.154851, mean_eps: 0.898300\n",
      " 1837/10000: episode: 3, duration: 1208.125s, episode steps: 577, steps per second:   0, episode reward: 190.000, mean reward:  0.329 [ 0.000, 10.000], mean action: 3.943 [0.000, 8.000],  loss: 1.042983, mean_q: 3.873156, mean_eps: 0.860680\n",
      " 2494/10000: episode: 4, duration: 1416.141s, episode steps: 657, steps per second:   0, episode reward: 210.000, mean reward:  0.320 [ 0.000, 10.000], mean action: 4.139 [0.000, 8.000],  loss: 0.689169, mean_q: 3.032516, mean_eps: 0.805150\n",
      " 3144/10000: episode: 5, duration: 1401.674s, episode steps: 650, steps per second:   0, episode reward: 220.000, mean reward:  0.338 [ 0.000, 10.000], mean action: 4.285 [0.000, 8.000],  loss: 0.555421, mean_q: 2.883585, mean_eps: 0.746335\n",
      " 3997/10000: episode: 6, duration: 1842.612s, episode steps: 853, steps per second:   0, episode reward: 690.000, mean reward:  0.809 [ 0.000, 200.000], mean action: 4.227 [0.000, 8.000],  loss: 7.712902, mean_q: 3.856132, mean_eps: 0.678700\n",
      " 4599/10000: episode: 7, duration: 1067.172s, episode steps: 602, steps per second:   1, episode reward: 270.000, mean reward:  0.449 [ 0.000, 10.000], mean action: 3.882 [0.000, 8.000],  loss: 8.018902, mean_q: 4.709589, mean_eps: 0.613225\n",
      " 5138/10000: episode: 8, duration: 889.500s, episode steps: 539, steps per second:   1, episode reward: 220.000, mean reward:  0.408 [ 0.000, 10.000], mean action: 3.529 [0.000, 8.000],  loss: 0.540929, mean_q: 4.175853, mean_eps: 0.561880\n",
      " 5736/10000: episode: 9, duration: 984.666s, episode steps: 598, steps per second:   1, episode reward: 200.000, mean reward:  0.334 [ 0.000, 10.000], mean action: 3.930 [0.000, 8.000],  loss: 0.308505, mean_q: 4.282729, mean_eps: 0.510715\n",
      " 6361/10000: episode: 10, duration: 1027.461s, episode steps: 625, steps per second:   1, episode reward: 250.000, mean reward:  0.400 [ 0.000, 10.000], mean action: 3.680 [0.000, 8.000],  loss: 0.362302, mean_q: 3.871190, mean_eps: 0.455680\n",
      " 7015/10000: episode: 11, duration: 1373.237s, episode steps: 654, steps per second:   0, episode reward: 230.000, mean reward:  0.352 [ 0.000, 10.000], mean action: 4.164 [0.000, 8.000],  loss: 0.317466, mean_q: 4.644459, mean_eps: 0.398125\n",
      " 7557/10000: episode: 12, duration: 1006.470s, episode steps: 542, steps per second:   1, episode reward: 250.000, mean reward:  0.461 [ 0.000, 10.000], mean action: 4.087 [0.000, 8.000],  loss: 0.347582, mean_q: 5.870572, mean_eps: 0.344305\n",
      " 8487/10000: episode: 13, duration: 1524.732s, episode steps: 930, steps per second:   1, episode reward: 750.000, mean reward:  0.806 [ 0.000, 200.000], mean action: 4.216 [0.000, 8.000],  loss: 7.066482, mean_q: 4.111273, mean_eps: 0.278065\n",
      " 9296/10000: episode: 14, duration: 1322.886s, episode steps: 809, steps per second:   1, episode reward: 420.000, mean reward:  0.519 [ 0.000, 10.000], mean action: 4.748 [0.000, 8.000],  loss: 3.871122, mean_q: 5.511116, mean_eps: 0.199810\n",
      " 9892/10000: episode: 15, duration: 973.007s, episode steps: 596, steps per second:   1, episode reward: 340.000, mean reward:  0.570 [ 0.000, 10.000], mean action: 4.943 [0.000, 8.000],  loss: 0.482729, mean_q: 5.265884, mean_eps: 0.136585\n",
      "done, took 16776.872 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f21c26deb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-84a0704920ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'episode_reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dqn' is not defined"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=5, visualize=True, verbose=2)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0753799cc516>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dqn_weights.h5f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dqn' is not defined"
     ]
    }
   ],
   "source": [
    "dqn.save_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f21da490f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
